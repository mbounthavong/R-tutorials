---
title: "Logistic regression in R"
author: "Mark Bounthavong"
date: "12/4/2021"
output: html_document
---

## Introduction
When you have a binary outcome (Yes/No), you can use a chi square test to compare the differences in proportions across $n$ number of groups. For instance, if you had two groups (exposed and unexposed) and a binary outcome (event and no event), you can create a 2 x 2 contingency table and use a chi square test to test if there is a difference in the frequency or proportion in the outcome across the two groups. However, this will not get you the magnitude of the differences, the direction of the difference, nor the uncertainty with the differences. 

```{r , echo=FALSE, fig.cap="Figure caption: 2 x 2 contingency table", out.width = '50%'}
knitr::include_graphics("Figure1.jpg")
```

Alternative measures of association include risk ratio and odds ratio. I wrote a tutorial on how to do this in R, which you can find at this [link](https://rpubs.com/mbounthavong/epitools_confounding_interaction). However, in this tutorial, we'll go over how we can leverage the odds ratio particularly when it comes to constructing a logistic regression model to make predictions. 

## Logistic regression model
The structural form of the logistic regression model:

<div align="center">$logit( E[Y_i | X_i]) = logit(p_i) = ln(\frac{p_i}{1 - p_i}) = \beta_0 + \beta_1 X_{1i} + \epsilon$</div> \

Typical notation of the linear regression include:

* $Y_i$ denotes the outcome (or dependent) variable for subject $i$; this is a binary variable
* $X_{1i}$ denotes the predictor of interest or the independent variable ($X_1$) for subject $i$
* $\beta_0$ denotes the Y-intercept when $X$ is zero; this is not informative for logistic regression models
* $\beta_1$ denotes the slope or the change in $Y$ with a 1-unit change in $X$
* $p_{i}$ denotes the probability of the event occurring
* $\epsilon$ denotes the errors

The logistic regression model is a predictive model for binary data. It is also known as a classification model. Hence, the logistic regression model can generate probabilities that a sample will have the discrete outcome given an input variable(s). The logistic regression model uses maximum likelihood estimation (MLE) which is a conditional probability that classifies the outcome if a certain threshold is met (e.g,. > 0.50). Hence, the probability range of a logistic regression model is between 0 and 1. The figure below provides an example of a logistic function. It uses a logit function to model a binary outcome. The logit function is the natural log of the odds. For instance, if the probability is > 0.5, the logit function is positive, if the probability is < 0.5, the logit function is negative.\

Unlike the 2 x 2 contingency table setup, the logistic regression allows for continuous and categorical variables as the independent or predictor variables. This allows for easy interpretation particularly with continuous data. Additionally, the logistic regression can include multiple predictors which can be controlled or adjusted in a multivariable logistic regression model. 

```{r, message = FALSE, warning = FALSE}
library(LaplacesDemon)
x <- -10:10
prob <- invlogit(x)
plot(x, prob, type = "l", main = "Logistic regression function plot", ylab = "Probability", xlab = "Values of X")
```

## Motivating Example - Logistic regression {.tabset}
### Data and libraries
We'll use the `mtcar` data to build our logistic regression model.

```{r, message = FALSE, warning = FALSE} 
#### Load the libraries
library("ggplot2")
library("gmodels")
library("epitools")
library("tidyverse")

### Create factors
data1 <- within(mtcars, {
           vs <- factor(vs, labels = c("V", "S"))
           am <- factor(am, labels = c("automatic", "manual"))
})
head(data1)
```

### 2x2 contingency table
Let's look at the 2x2 contingency table between transmission types `am` (0 = automatic and 1 = manual) with the engine types `vs` (0 = V-shaped and 1 = straight).\

Among vehicles with a V-shaped engine, 33% (N = 6) had a manual transmission and 67% (N = 12) had an automatic transmission. According to the Pearson's chi square test (p = 0.34), this difference was not statistically significant. 

```{r, message = FALSE, warning = FALSE}
CrossTable(data1$vs, data1$am, chisq = TRUE, missing.include = TRUE)
```

### Odds ratio calculation
We can calculate the crude odds ratio. Using the odds ratio, vehicles with a "V" engine had a 2 times higher odds of having an automatic transmission (95% CI: 0.47, 8.40) compared to vehicles with a straight engine. 

```{r, message = FALSE, warning = FALSE}
oddsratio(data1$vs, data1$am, conf.level = 0.95, method = "wald")
```
### Logistic regression in R
We can create a crude logistic regression model to estimate the odds ratio. We set the transmission type `am` as the dependent variable and the engine type `vs` as the independent variable. The `glm()` command generates coefficients that are interpreted as the log odds of the event occuring. We need to exponentiate this to get the odds ratio using the `exp()` command.\

According to the logistic regression model, vehicles with an "V" engine had a 2.0 times higher odds of having an automatic transmission (95% CI: 0.48, 8.76) compared to vehicles with a straight engine; this is not statistically significant since the odds ratio crosses the null or OR = 1. 

```{r, message = FALSE, warning = FALSE}
logit1 <- glm(formula = am ~ vs, data = data1, family = "binomial"(link = "logit"))
summary(logit1)

### Generate the 95% CI
confint(logit1)

### Exponentiate the coefficients
exp(coef(logit1))     ### Odds ratio
exp(confint(logit1))  ### 95% CI (odds ratio)
```

## {-}



## Multivariable logistic regression model {.tabset}
The structural form of the multivariable logistic regression model (this example uses two `X` variables):

<div align="center">$logit( E[Y_i | X_i]) = logit(p_i) = ln(\frac{p_i}{1 - p_i}) = \beta_0 + \beta_1 X_{1i} + + \beta_2 X_{2i} + \epsilon$</div> \

Since the logistic regression model can include both continuous and categorical predictors, we can add the engine type `vs` (V versus straight engine) and vehicle weight `wt`.\

<div align="center">$logit( E[Y_i | X_i]) = logit(p_i) = ln(\frac{p_i}{1 - p_i}) = \beta_0 + \beta_1 (vs)_{i} + + \beta_2 (wt)_{i} + \epsilon$</div>\

where `vs` is the engine type and `wt` is the vehicle weight. 

### Descriptives
There is a total of 32 observations; N = 19 vehicles have automatic transmission and N =13 have manual transmission. There are 18 (56.2%) vehicles with V-type engines and 14 (43.8%) vehicles with straight type engines. The average weight of a vehicles with automatic transmission is 3.77 units (SD, 0.78) and for vehicles with manual transmission, the average weight is 2.41 units (SD, 0.62); this difference was statistically significant (p<0.001). 

```{r, message = FALSE, warning = FALSE}
library("dplyr")

### Get the n for transmission and engine types
CrossTable(data1$am)
CrossTable(data1$vs)

### Get the n, mean, and sd of weight by transmission type.
group_by(data1, am) %>%
  summarise(
    count = n(),
    mean = mean(wt, na.rm = TRUE),
    sd = sd(wt, na.rm = TRUE)
  )

### Compare the average vehicle weight by transmission type. 
t.test(data1$wt ~ data1$am, alternative = "two.sided", paired = FALSE, var.equal = FALSE, conf.level = 0.95)
```

### Multivariable logistic regression model
We add the `vs` and `wt` variables as predictors in the logistic regression model. The output will be in log odds, but we can exponentiate this using the `exp()` command.\

We get very different results from the first logistic regression model. In this multivariable logistic regression model, the association between engine type `vs` and transmission type `am` is much lower (OR = 0.01; 95% CI: 0.000005, 0.048) controlling for vehicle weight `wt` Controlling for the vehicle's weight reduced odds of the association between engine type `vs` and transmission type `am`. In the previous logistic regression model, vehicles with a V-type engine has a 2.0 times higher odds of having an automatic transmission. However, when adjusting for the vehicles weight, this association is no longer significant. 

```{r, message = FALSE, warning = FALSE}
logit2 <- glm(formula = am ~ vs + wt, data = data1, family = "binomial"(link = "logit"))
summary(logit2)

### Generate the 95% CI
confint(logit2)

### Exponentiate the coefficients
exp(coef(logit2))     ### Odds ratio
exp(confint(logit2))  ### 95% CI (odds ratio)
```

### Comparison between models
We compare the odds ratio between the crude and adjusted logistic regression models. 

```{r , echo=FALSE, fig.cap="Figure caption: Comparison between crude and adjustment logistic regression models", out.width = '75%'}
knitr::include_graphics("Figure2.jpg")
```

## {-}

## Motivating example -- diabetes dataset {.tabset}
Let's use the `diabetes.csv` data set to construct a logistic regression model. You can download the data [here](https://www.dropbox.com/s/c3tjrwhtre23701/diabetes.csv?dl=0). 

### Descriptives
There are 768 observations in the `diabetes.data` data frame. 

```{r, message = FALSE, warning = FALSE}
library("psych")

## Import file
diabetes.data <- read.csv("diabetes.csv", header = TRUE)
head(diabetes.data)

### Descriptive of diabetes.data
describeBy(diabetes.data)
```

### Create categorical variables
Currently, the data frame has a variable called `Pregnancies` that contains the number of pregnancies each subject has. We are only interested in whether or not they have had a pregnancy, so we'll create a binary variable called `group`. We added labels to the 0s an 1s and call this new data frame `data2`. There was a total of 657 (85.5%) subjects with a history of pregnancies and 111 (14.5%) subjects with no history of pregnancy. 

```{r, message = FALSE, warning = FALSE}
#### Generate groups based on pregnancies (Group 1 = 0, Group 2 = 1-5, Group 3 = >5 pregnancies)
diabetes.data$group[diabetes.data$Pregnancies == 0] = 0 
diabetes.data$group[diabetes.data$Pregnancies > 0] = 1 
CrossTable(diabetes.data$group)

### Create factors
data2 <- within(diabetes.data, {
           group <- factor(group, labels = c("No history of pregnancies", "History of pregnancies"))
})

CrossTable(data2$group)
```

### Crude Logistic Regression Model
We create a crude logistic regression model to evaluate the association of the subject's age `Age` on history of pregnancy `group`. Baesd on the crude logistic regression model, a 1-unit increase in age was associated with a 7% increase in the odds of having a history of pregnancy (95% CI: 1.05, 1.10), which is statistically significant. 

```{r, message = FALSE, warning = FALSE}
logit3 <- glm(group ~ Age, data = data2, family = "binomial"(link = "logit"))
summary(logit3)
confint(logit3)

### Exponentiate the coefficients
exp(coef(logit3))
exp(confint(logit3))
```

### Multivariable logistic regression model
We can add potential confounders such as BMI, glucose, and skin thickness to the logistic regression model. This will generate model estimates that will be adjusting for these other predictors. 

```{r, message = FALSE, warning = FALSE}
logit3 <- glm(group ~ Age + BMI + Glucose + SkinThickness, data = data2, family = "binomial"(link = "logit"))
summary(logit3)
confint(logit3)

### Exponentiate the coefficients
exp(coef(logit3))
exp(confint(logit3))
```

### Models comparisons
The odds ratio describing the association between Age and History of pregnancy did not change much between the crude and adjusted logistic regression model. 

```{r , echo=FALSE, fig.cap="Figure caption: Comparison between crude and adjustment logistic regression models", out.width = '75%'}
knitr::include_graphics("Figure3.jpg")
```

## Conclusions
Logistic regression models allow us to estimate the association between a binary variable with a predictor variables that can be continuous or categorical. Additionally, the logistic regression model allows us to include more than one predictor variable thereby controlling for their effects. This is useful when potential confounders are present and the researcher wants to adjust for them.

## Contact
These tutorials are a work in progress, so they may contain errors. I highly recommend you seek out statistical consult if you plan on running any analysis meant for publication. Any comments and suggestions can be sent to: internal.validity.blog@gmail.com

